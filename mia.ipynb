{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)"
      ],
      "metadata": {
        "id": "o0cH2x1JR69g",
        "outputId": "9ccb5224-ee25-4507-a08b-887fc469f84c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "o-bj7HOESAEd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "\n",
        "# Load pre-trained MobileNetV2 model\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define transforms to preprocess the input image\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load and preprocess the input image\n",
        "image_path = \"/content/tiger.webp\"\n",
        "image = Image.open(image_path)\n",
        "input_tensor = preprocess(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Move the input and model to GPU for faster computation if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "# Perform prediction\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "\n",
        "# Post-process the output (convert logits to probabilities)\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "9D3A46wM-1Kv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities"
      ],
      "metadata": {
        "id": "LQl1TzQ5LV6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted class index\n",
        "predicted_class_idx = torch.argmax(probabilities).item()\n",
        "\n",
        "print(\"Predicted class index:\", predicted_class_idx)\n",
        "print(\"Probability:\", probabilities[predicted_class_idx].item())"
      ],
      "metadata": {
        "id": "sV9tFLviLUZ0",
        "outputId": "8b6a0106-13a3-49d7-f962-feb752d290ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: 292\n",
            "Probability: 0.9457566738128662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "categories[predicted_class_idx]"
      ],
      "metadata": {
        "id": "w6wZsfgTOA6R",
        "outputId": "69b7c620-c8ef-4a98-d40c-794471a9e392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiger'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = model.classifier[-1].state_dict()['weight'].shape[0]\n",
        "\n",
        "print(\"Number of classes:\", class_labels)"
      ],
      "metadata": {
        "id": "AnTvwVM8Azo6",
        "outputId": "5808d1c8-b623-4f02-84ce-083f173f7797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 1280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "metadata": {
        "id": "JseGLtXkDWWR",
        "outputId": "e74af25c-fe60-4c33-b1b3-36b82feff879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-17 12:34:51--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10472 (10K) [text/plain]\n",
            "Saving to: ‘imagenet_classes.txt’\n",
            "\n",
            "\rimagenet_classes.tx   0%[                    ]       0  --.-KB/s               \rimagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-17 12:34:51 (73.4 MB/s) - ‘imagenet_classes.txt’ saved [10472/10472]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "categories"
      ],
      "metadata": {
        "id": "egaOHXJQJuvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"tiger\" in categories:\n",
        "  print(\"pongal\")"
      ],
      "metadata": {
        "id": "kZJceOf1Q6H3",
        "outputId": "de6a8b86-4b7a-45d4-893d-9ed52188ec23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pongal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp -r /content/drive/MyDrive/mia  /content/mia"
      ],
      "metadata": {
        "id": "X2qWGH3bWWYa"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='/content/mia', transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load pretrained MobileNetV2\n",
        "model = mobilenet_v2(pretrained=True)\n",
        "# Modify the model's output layer for single class classification\n",
        "num_ftrs = model.classifier[-1].in_features\n",
        "model.classifier[-1] = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "# Optionally freeze the pretrained layers\n",
        "# for param in model.features.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 50\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())  # Squeeze outputs if necessary\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "FQyWvo6OVgiI",
        "outputId": "9f5dbc16-d815-4d92-9c90-1d666951fc63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.9111\n",
            "Epoch [2/50], Loss: 0.5661\n",
            "Epoch [3/50], Loss: 0.3245\n",
            "Epoch [4/50], Loss: 0.1505\n",
            "Epoch [5/50], Loss: 0.0882\n",
            "Epoch [6/50], Loss: 0.0555\n",
            "Epoch [7/50], Loss: 0.0299\n",
            "Epoch [8/50], Loss: 0.0197\n",
            "Epoch [9/50], Loss: 0.0137\n",
            "Epoch [10/50], Loss: 0.0093\n",
            "Epoch [11/50], Loss: 0.0069\n",
            "Epoch [12/50], Loss: 0.0050\n",
            "Epoch [13/50], Loss: 0.0037\n",
            "Epoch [14/50], Loss: 0.0029\n",
            "Epoch [15/50], Loss: 0.0024\n",
            "Epoch [16/50], Loss: 0.0019\n",
            "Epoch [17/50], Loss: 0.0014\n",
            "Epoch [18/50], Loss: 0.0014\n",
            "Epoch [19/50], Loss: 0.0012\n",
            "Epoch [20/50], Loss: 0.0010\n",
            "Epoch [21/50], Loss: 0.0009\n",
            "Epoch [22/50], Loss: 0.0008\n",
            "Epoch [23/50], Loss: 0.0007\n",
            "Epoch [24/50], Loss: 0.0006\n",
            "Epoch [25/50], Loss: 0.0006\n",
            "Epoch [26/50], Loss: 0.0005\n",
            "Epoch [27/50], Loss: 0.0005\n",
            "Epoch [28/50], Loss: 0.0005\n",
            "Epoch [29/50], Loss: 0.0004\n",
            "Epoch [30/50], Loss: 0.0004\n",
            "Epoch [31/50], Loss: 0.0004\n",
            "Epoch [32/50], Loss: 0.0004\n",
            "Epoch [33/50], Loss: 0.0004\n",
            "Epoch [34/50], Loss: 0.0003\n",
            "Epoch [35/50], Loss: 0.0003\n",
            "Epoch [36/50], Loss: 0.0003\n",
            "Epoch [37/50], Loss: 0.0003\n",
            "Epoch [38/50], Loss: 0.0003\n",
            "Epoch [39/50], Loss: 0.0002\n",
            "Epoch [40/50], Loss: 0.0003\n",
            "Epoch [41/50], Loss: 0.0003\n",
            "Epoch [42/50], Loss: 0.0003\n",
            "Epoch [43/50], Loss: 0.0002\n",
            "Epoch [44/50], Loss: 0.0003\n",
            "Epoch [45/50], Loss: 0.0002\n",
            "Epoch [46/50], Loss: 0.0002\n",
            "Epoch [47/50], Loss: 0.0002\n",
            "Epoch [48/50], Loss: 0.0002\n",
            "Epoch [49/50], Loss: 0.0002\n",
            "Epoch [50/50], Loss: 0.0002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "# Define transforms with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset with train transforms\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='/content/mia', transform=train_transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Modify the model's output layer for single class classification\n",
        "num_ftrs = model.classifier[-1].in_features\n",
        "model.classifier[-1] = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "# Optionally freeze the pretrained layers\n",
        "# for param in model.features.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Define number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())  # Squeeze outputs if necessary\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predictions = (torch.sigmoid(outputs) > 0.5).int()\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "6xMCunTzgB2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# Define transformations for input data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the input image\n",
        "    transforms.ToTensor(),           # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image\n",
        "])\n",
        "\n",
        "# Load and preprocess your input data\n",
        "input_image = Image.open('/content/448464640.webp')\n",
        "input_tensor = transform(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "\n",
        "# Interpret the output\n",
        "probabilities = torch.softmax(output[0], dim=0)\n",
        "predicted_class = torch.argmax(probabilities).item()\n",
        "\n",
        "if predicted_class == 0:\n",
        "  print(\"mia\")\n",
        "else:\n",
        "  print(\"not mia\")"
      ],
      "metadata": {
        "id": "0wcN_AleVgfL",
        "outputId": "559ac65a-ce4b-4b2c-af79-5036e9bc0df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define transforms for preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and preprocess your input image\n",
        "image_path = '/content/mia-khalifa-grinning-zakg54fkgw8hatlm.webp'\n",
        "image = Image.open(image_path)\n",
        "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_tensor)\n"
      ],
      "metadata": {
        "id": "BKuxvFHAVgcn",
        "outputId": "919eeb34-6a82-44ac-fb6f-dba8d56efe06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not mia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "id": "IiMcEk08eOZ3",
        "outputId": "0fe087bc-22eb-4fb8-be4d-352199df1b57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00020826698164455593"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing\n",
        "probabilities = torch.sigmoid(outputs)  # Assuming your model output is logits, and you're using BCEWithLogitsLoss\n",
        "prediction = probabilities.item()  # Assuming you're interested in a single class\n",
        "\n",
        "# Decision\n",
        "threshold = 0.5  # Adjust this threshold based on your model's behavior and your requirements\n",
        "if prediction >= threshold:\n",
        "    print(\"mia\")\n",
        "else:\n",
        "    print(\"not mia\")"
      ],
      "metadata": {
        "id": "nHdhMvGUd8EZ",
        "outputId": "58e29ab6-9b8f-4507-9e33-356a7b923f78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not mia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Load and preprocess the input image\n",
        "def preprocess_image(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    image = Image.open(image_path)\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "    return image\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Perform inference\n",
        "input_image = preprocess_image('/content/mia-khalifa-grinning-zakg54fkgw8hatlm.webp')\n",
        "with torch.no_grad():\n",
        "    output = model(input_image)\n",
        "\n",
        "# Convert logits to class predictions\n",
        "prediction = (output > 0).int().item()  # Convert logits to binary prediction\n",
        "if prediction == 1:\n",
        "    print(\"The image belongs to your class.\")\n",
        "else:\n",
        "    print(\"The image does not belong to your class.\")\n"
      ],
      "metadata": {
        "id": "UmSB5NpmVgZs",
        "outputId": "1e476c55-83ce-436a-b94c-d0026139078c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image does not belong to your class.\n"
          ]
        }
      ]
    }
  ]
}